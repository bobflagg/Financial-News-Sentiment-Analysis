{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fnsa.entity import ENScopeDetector\n",
    "from fnsa.lexicon import get_en2scope, Lexicon\n",
    "from fnsa.scope import DRScopeDetector, IFScopeDetector\n",
    "from fnsa.graph import make_graph\n",
    "from fnsa.util import *\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentences = load_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexicon = Lexicon(nlp)\n",
    "dr_detector = DRScopeDetector()\n",
    "if_detector = IFScopeDetector()\n",
    "en_detector = ENScopeDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_entity_scope(sentence, detectors=[dr_detector, en_detector, if_detector]):\n",
    "    print(sentence)\n",
    "    doc = lexicon(sentence)\n",
    "    show(doc, index=None, include_text=False, include_tree=True)\n",
    "    graph = make_graph(doc)\n",
    "    lex2tokens = build_lex2tokens(doc)\n",
    "    for detector in detectors: detector(doc, graph, lex2tokens)\n",
    "    en2scope = get_en2scope(doc)\n",
    "    for index, ent in enumerate(doc.ents): \n",
    "        if ent[0].i in en2scope:\n",
    "            tokens = [token for token in doc if token.i in en2scope[ent[0].i]]\n",
    "            tokens = sorted(tokens, key=lambda token: token.i)\n",
    "            print(\"%d. %s -->> %s\" % (index + 1, ent.text, \" \".join(annotate_token(token) for token in tokens)))\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for sentence in sentences: \n",
    "    doc = show_entity_scope(sentence, detectors=[dr_detector, en_detector, if_detector])\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:fsa]",
   "language": "python",
   "name": "conda-env-fsa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
