{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [How to make or get corpus of financial documents](https://stackoverflow.com/questions/32127265/how-to-make-or-get-corpus-of-financial-documents)\n",
    "- [Reuters-21578](http://www.daviddlewis.com/resources/testcollections/reuters21578/)\n",
    "- [Reuters-21578-Classification](https://github.com/giuseppebonaccorso/Reuters-21578-Classification)\n",
    "- [Financial News Dataset from Reuters](https://github.com/Danbo3004/financial-news-dataset)\n",
    "- [Reuters Dataset of Financial News Articles](https://github.com/Kriyszig/financial-news-data)\n",
    "- [Sentence-Level Sentiment Analysis of Financial News Using Distributed Text Representations and Multi-Instance Learning](https://arxiv.org/pdf/1901.00400.pdf)\n",
    "- [SentenceLevelSentimentFinancialNews](https://github.com/InformationSystemsFreiburg/SentenceLevelSentimentFinancialNews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fnsa.feature import FeatureExtractor\n",
    "from fnsa.lexicon import get_en2scope, Lexicon\n",
    "from fnsa.scope import DRScopeDetector, IFScopeDetector\n",
    "from fnsa.graph import make_graph\n",
    "from fnsa.util import *\n",
    "import os\n",
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "include_words = False\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "lexicon = Lexicon(nlp)\n",
    "dr_detector = DRScopeDetector()\n",
    "if_detector = IFScopeDetector()\n",
    "extractor = FeatureExtractor(lexicon, detectors=[dr_detector, if_detector], include_words=include_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = '/opt/code/sentiment-analysis/data/fpb/FinancialPhraseBank-v1.0'\n",
    "fname = 'Sentences_AllAgree.txt'\n",
    "path = os.path.join(directory, fname)\n",
    "#os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2264 records.\n"
     ]
    }
   ],
   "source": [
    "sentiment2code = {'negative':-1, 'neutral':0, 'positive':1}\n",
    "\n",
    "records = []\n",
    "with open(path, mode='r', encoding='Windows-1252') as ifp:\n",
    "    cnt = 0\n",
    "    for line in ifp:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            text, sentiment = line.split('@')\n",
    "            records.append((text, sentiment2code[sentiment]))\n",
    "            cnt += 1\n",
    "print(\"Loaded %d records.\" % cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2264/2264 [00:22<00:00, 100.87it/s]   | 1/2264 [00:00<03:50,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 2.78 s, total: 1min 28s\n",
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "directory = './data'\n",
    "fname = 'all-agree.tsv'\n",
    "if include_words: fname = 'all-agree-with-words.tsv'\n",
    "path = os.path.join(directory, fname)\n",
    "with open(path, mode='w', encoding='UTF-8') as ofp:\n",
    "    ofp.write(\"Sentiment\\tSentence\\tFeatures\\n\")\n",
    "    for record in tqdm(records):\n",
    "        text, code = record\n",
    "        doc, features = extractor(text)\n",
    "        features = \" \".join([feature.lower() for feature in features])\n",
    "        ofp.write(\"%d\\t%s\\t%s\\n\" % (code, text, features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sentence-Level Sentiment Analysis of Financial News Test Dataset\n",
    "\n",
    "There is a small test dataset related to the paper\n",
    "\n",
    " > [Sentence-Level Sentiment Analysis of Financial News](https://arxiv.org/pdf/1901.00400.pdf)<br/>\n",
    " > Bernhard Lutz, Nicolas Prollochs and Dirk Neumann. (2018)\n",
    " \n",
    "available in the repo\n",
    "\n",
    " > [Sentence Level Sentiment Financial News Dataset](https://github.com/InformationSystemsFreiburg/SentenceLevelSentimentFinancialNews)\n",
    " \n",
    "which is useful for additional evaluation of our approach and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 records.\n"
     ]
    }
   ],
   "source": [
    "path = '/opt/code/github/SentenceLevelSentimentFinancialNews/adhoc_test.tsv'\n",
    "with open(path) as ifp: lines = ifp.readlines()\n",
    "records = []\n",
    "cnt = -1\n",
    "for line in lines:\n",
    "    cnt += 1\n",
    "    if cnt == 0: continue\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        sentence, code = line.split('\\t')\n",
    "        sentence = sentence.strip()\n",
    "        code = int(code)\n",
    "        records.append((sentence, code))\n",
    "print(\"Loaded %d records.\" % cnt)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 84.93it/s]    | 2/1000 [00:00<00:53, 18.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.4 s, sys: 1.64 s, total: 44 s\n",
      "Wall time: 11.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "directory = './data'\n",
    "fname = 'sentence-level.tsv'\n",
    "if include_words: fname = 'sentence-level-with-words.tsv'\n",
    "path = os.path.join(directory, fname)\n",
    "with open(path, mode='w', encoding='UTF-8') as ofp:\n",
    "    ofp.write(\"Sentiment\\tSentence\\tFeatures\\n\")\n",
    "    for record in tqdm(records):\n",
    "        text, code = record\n",
    "        doc, features = extractor(text)\n",
    "        features = \" \".join([feature.lower() for feature in features])\n",
    "        ofp.write(\"%d\\t%s\\t%s\\n\" % (code, text, features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:fsa]",
   "language": "python",
   "name": "conda-env-fsa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
